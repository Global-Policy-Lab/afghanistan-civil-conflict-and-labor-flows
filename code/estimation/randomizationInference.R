#!/usr/bin/Rscript
args = commandArgs(trailingOnly = TRUE) 
####################### randomization inference #######################
suppressMessages(library(dplyr))

#### harvest dates
bestDatesLong <- readRDS("/data/afg_satellite/bestdates/6-22-22bestDatesLong_M4.rds") %>%
  select(distIDs, year, maxDate) %>%
  rename("distID" = "distIDs",
         "date" = "maxDate")

numericArg <- as.numeric(args[1])

set.seed(numericArg)
randomNumbers <- sample(0:180, nrow(bestDatesLong), replace = TRUE)
bestDatesLong <- bestDatesLong %>%
  mutate(date = as.Date(paste0(year, "-01-01")) + randomNumbers)

bestDatesWide <- bestDatesLong %>%
  tidyr::pivot_wider(names_from = "year",
                     values_from = "date",
                     names_prefix = "year_")

ddOutcomes <- readRDS("/home/xtai/climate/3-8-23migrationCleanCode/output/6-5-23ddOutcomes_2020.rds") %>% # says 6/5/23 but updated 7/24 after Shikhar fix 
  select(-starts_with("baseline"),
         -starts_with("year"))

ddOutcomes <- ddOutcomes %>%
  left_join(bestDatesWide,
            by = c("district_id" = "distID")) %>%
  mutate(across(where(is.character), as.Date)) %>%
  data.frame()

# coefs_yyyy: 1 month before to 2 months after
# baseline_yyyy: anything within 4 months before to 2 months after
for (i in 2014:2020) {
  ddOutcomes[, paste0("baseline_", i)] <- ifelse(ddOutcomes$date >= ddOutcomes[, paste0("year_", i)] - lubridate::days(120) & 
                                                   ddOutcomes$date <= ddOutcomes[, paste0("year_", i)] + lubridate::days(90),# change from 60 to 90
                                                 1, 0)
}

source("../makeDataHelper.R") # outFun is in here

#########
suffix <- "" # "_rB" # "_rA"
outDTF <- outFun(ddOutcomes, suffix)

# 3-4 minutes
# 2:13 PM - 2:20

#################  Part 3: make excess harvest in-migration 
distIDs <- unique(outDTF$distid)
newResponse <- data.frame(distid = distIDs, 
                          year = c(rep(2014, length(distIDs)), 
                                   rep(2015, length(distIDs)),
                                   rep(2016, length(distIDs)),
                                   rep(2018, length(distIDs)),
                                   rep(2019, length(distIDs)),
                                   rep(2020, length(distIDs))), 
                          maxIn = NA)

for (tmpYear in c(2014:2016, 2018:2020)) {
  for (i in 1:length(distIDs)) {
    tmpDTF <- outDTF %>%
      filter(year == tmpYear & distid == distIDs[i])
    
    if (nrow(tmpDTF) > 0) {
      tmpDTF <- tmpDTF %>%
        filter(daysFromPeak %in% 15:35) #%>%
      # filter(daysFromPeak %in% 1:45) #%>%
      newDTF <- data.frame(daysFromPeak = 15:35) %>%
        # newDTF <- data.frame(daysFromPeak = 1:45) %>%
        left_join(tmpDTF %>% select(daysFromPeak, estimate),
                  by = "daysFromPeak")
      newDTF$rollingMean <- NA
      daysToConsider <- 7 #14
      for (ii in 1:(nrow(newDTF) - daysToConsider + 1)) {
        newDTF$rollingMean[ii] <- mean(newDTF$estimate[ii:(ii + daysToConsider - 1)])
      }
      if (sum(!is.na(newDTF$rollingMean)) > 0) {
        newResponse[which(newResponse$distid == distIDs[i] & newResponse$year == tmpYear), "maxIn"] <- max(newDTF$rollingMean, na.rm = TRUE) 
      }
    }
  }
}

# saveRDS(newResponse, file = paste0("/home/xtai/climate/3-8-23migrationCleanCode/8-9-24review1/random/random_", sprintf("%02d", numericArg), ".rds"))
saveRDS(newResponse, file = paste0("/home/xtai/climate/3-8-23migrationCleanCode/8-9-24review1/random/random_", sprintf("%03d", numericArg), ".rds"))

# nohup ./randomizationInference.R 2 & 
