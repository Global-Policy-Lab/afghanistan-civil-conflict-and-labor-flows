{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "dask_n_workers = 32\n",
    "dask_worker_memory_limit = 16\n",
    "dask_threads_per_worker = 16\n",
    "cluster = LocalCluster(dashboard_address = 'localhost:7920', \n",
    "                       n_workers = dask_n_workers, \n",
    "                       processes = True, \n",
    "                       threads_per_worker = dask_threads_per_worker,\n",
    "                       memory_limit = str(dask_worker_memory_limit) + 'GB', \n",
    "                       local_directory = \"/path/to/dask-worker-space\")\n",
    "client = Client(cluster)\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import shutil\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import time\n",
    "from datetime import timedelta  \n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Home Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert daily modal location data to row major format\n",
    "# Input schema: \n",
    "# phoneHash, day, daily_modal_location\n",
    "# Output schema (if we have 10 days of data):\n",
    "# phoneHash, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "def convertToRowMajorFormat(data, first_day, last_day):\n",
    "\n",
    "    data['day_series'] = (data.day - first_day).dt.days\n",
    "\n",
    "    # make dataframe wide form - one row per user\n",
    "    data = data.pivot_table(index='phoneHash1', columns='day_series', values='daily_modal_location')\n",
    "\n",
    "    # uncategorize column names\n",
    "    existing_days = data.columns.tolist()\n",
    "    existing_days = list(map(str, existing_days))\n",
    "    data.columns = existing_days\n",
    "\n",
    "    # add missing columns\n",
    "    total_days = (last_day - first_day).days + 1\n",
    "    day_series_range = np.arange(0, total_days, 1)\n",
    "    day_series_range = list(map(str, day_series_range))\n",
    "\n",
    "    missing_days = set(day_series_range).difference(set(existing_days))\n",
    "    for day in missing_days:\n",
    "        data[day] = np.NaN\n",
    "\n",
    "    # order columns\n",
    "    data = data[day_series_range]\n",
    "\n",
    "    # convert column names to int\n",
    "    data.columns = np.arange(0, total_days, 1)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Convert location data to column major format\n",
    "# Input schema (if we have 10 days of data):\n",
    "# phoneHash, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "# Output schema: \n",
    "# phoneHash, day, daily_modal_location\n",
    "def convertToColumnMajorFormat(data, first_day, last_day):\n",
    "    \n",
    "    data = data.reset_index()\n",
    "    \n",
    "    # get column names based on first_day and last_day of daily modal locations\n",
    "    total_days = (last_day - first_day).days + 1\n",
    "    day_series_range = np.arange(0, total_days, 1)\n",
    "    \n",
    "    # unpivot table\n",
    "    data = ddf.reshape.melt(data, id_vars=['phoneHash1'], value_vars=day_series_range, var_name='day_series', value_name='home_location')\n",
    "\n",
    "    # convert day_series to day\n",
    "    data['day'] = first_day + ddf.to_timedelta(data.day_series, unit=\"d\")\n",
    "\n",
    "    # drop NaN location values\n",
    "    data = data.dropna()[['phoneHash1', 'day', 'home_location']]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# drop location segments which have low proportion of actual raw locations\n",
    "# input: \n",
    "# raw_locations: 2, np.NaN, np.NaN, 3, 3, np.NaN, np.NaN\n",
    "# inferred_locations: 2, 2, np.NaN, 3, 3, 3, np.NaN\n",
    "# segment_proportion_days_threshold: 0.6\n",
    "#\n",
    "# output: np.NaN, np.NaN, np.NaN, 3, 3, 3, np.NaN\n",
    "def dropLowPropSegments(raw_locations, inferred_locations, segment_proportion_days_threshold):\n",
    "    \n",
    "    # check if segment satisfies segment_proportion_days_threshold\n",
    "    def removeSegmentIfInvalid(raw_locations, inferred_locations, segment_start_day, segment_end_day):\n",
    "        \n",
    "        #print(\"validateSegment: \\n\", inferred_locations.tolist())\n",
    "        #print(\"segment_start_day: \", segment_start_day)\n",
    "        #print(\"segment_end_day: \", segment_end_day)\n",
    "        \n",
    "        segment = raw_locations[segment_start_day:segment_end_day]\n",
    "\n",
    "        segment_length = len(segment)\n",
    "        days_with_cdr_activity = len(segment[segment.notna()])\n",
    "                \n",
    "        prop_days_with_cdr_activity = days_with_cdr_activity/segment_length\n",
    "        \n",
    "        if(prop_days_with_cdr_activity < segment_proportion_days_threshold):\n",
    "\n",
    "            # remove segment if it does not satisfy segment proportion day criteria\n",
    "            inferred_locations[segment_start_day:segment_end_day] = np.NaN\n",
    "\n",
    "    \n",
    "    # get non-null location series\n",
    "    locations = inferred_locations.dropna()\n",
    "    location_indices = locations.index.tolist()\n",
    "\n",
    "    # return if all locations are null\n",
    "    if(locations.empty):\n",
    "        return inferred_locations\n",
    "    \n",
    "    # get index of start of (first) segment\n",
    "    segment_start_day = location_indices[0]\n",
    "    # runner to track previous non-null location day\n",
    "    previous_active_day = segment_start_day\n",
    "\n",
    "    # iterate through non-null days and remove invalid segments\n",
    "    for iterator in range(0, len(location_indices)):\n",
    "        \n",
    "        current_day = location_indices[iterator]\n",
    "                \n",
    "        if((current_day - previous_active_day) > 1):\n",
    "                \n",
    "                segment_end_day = location_indices[iterator - 1] + 1\n",
    "\n",
    "                removeSegmentIfInvalid(raw_locations, inferred_locations, segment_start_day, segment_end_day)\n",
    "\n",
    "                segment_start_day = current_day\n",
    "                \n",
    "        previous_active_day = current_day\n",
    "\n",
    "    segment_end_day = location_indices[iterator] + 1\n",
    "\n",
    "    removeSegmentIfInvalid(raw_locations, inferred_locations, segment_start_day, segment_end_day)\n",
    "        \n",
    "    return inferred_locations\n",
    "\n",
    "# drop location segments which are smaller than min_segment_length\n",
    "# input: \n",
    "# location_series: 2, 2, np.NaN, 3, 3, 3, np.NaN\n",
    "# min_segment_length: 3\n",
    "#\n",
    "# output: np.NaN, np.NaN, np.NaN, 3, 3, 3, np.NaN\n",
    "def dropSmallSegments(location_series, min_segment_length):\n",
    "    \n",
    "    # check if segment length is smaller than minimum segment length permitted\n",
    "    def removeSegmentIfInvalid(location_series, segment_start_day, segment_end_day):\n",
    "        \n",
    "        #print(\"validateSegment: \\n\", location_series.tolist())\n",
    "        #print(\"segment_start_day: \", segment_start_day)\n",
    "        #print(\"segment_end_day: \", segment_end_day)\n",
    "\n",
    "        if(segment_end_day - segment_start_day < min_segment_length):\n",
    "            \n",
    "            # remove segment if its smaller than min_segment_length\n",
    "            location_series[segment_start_day:segment_end_day] = np.NaN\n",
    "\n",
    "\n",
    "    # get non-null location series            \n",
    "    locations = location_series.dropna()\n",
    "    location_indices = locations.index.tolist()\n",
    "\n",
    "    # return if all locations are null    \n",
    "    if(locations.empty):\n",
    "        return location_series\n",
    "    \n",
    "    # get index of start of (first) segment\n",
    "    segment_start_day = location_indices[0]\n",
    "    # runner to track previous non-null location day\n",
    "    previous_active_day = segment_start_day\n",
    "    \n",
    "    # iterate through non-null days and remove invalid segments\n",
    "    for iterator in range(0, len(location_indices)):\n",
    "        \n",
    "        current_day = location_indices[iterator]\n",
    "                \n",
    "        if((current_day - previous_active_day) > 1):\n",
    "                \n",
    "            segment_end_day = location_indices[iterator - 1] + 1\n",
    "\n",
    "            removeSegmentIfInvalid(location_series, segment_start_day, segment_end_day)\n",
    "\n",
    "            segment_start_day = current_day\n",
    "                \n",
    "        previous_active_day = current_day\n",
    "\n",
    "    segment_end_day = location_indices[iterator] + 1\n",
    "\n",
    "    removeSegmentIfInvalid(location_series, segment_start_day, segment_end_day)\n",
    "        \n",
    "    return location_series    \n",
    "\n",
    "# drop segment / stays that are smaller than threshold\n",
    "# min_segment_length: 3\n",
    "# input df: \n",
    "# 3, 3, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN\n",
    "# np.NaN, np.NaN, 2, 2, 2, np.NaN, np.NaN, np.NaN\n",
    "#\n",
    "# output df: \n",
    "# np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, np.NaN\n",
    "# np.NaN, np.NaN, 2, 2, 2, np.NaN, np.NaN, np.NaN\n",
    "def dropSmallStays(segments_df, min_segment_length):\n",
    "    \n",
    "    segments_df = segments_df.apply(lambda location_series: dropSmallSegments(location_series, min_segment_length), axis = 1)\n",
    "    \n",
    "    return segments_df\n",
    "        \n",
    "# remove overlaps b/w concurrent segments\n",
    "# input df: \n",
    "# 3, 3, 3, np.NaN, 3, 3, 3, np.NaN\n",
    "# np.NaN, 2, 2, 2, np.NaN, np.NaN, np.NaN, np.NaN\n",
    "#\n",
    "# output df: \n",
    "# 3, np.NaN, np.NaN, np.NaN, 3, 3, 3, np.NaN\n",
    "# np.NaN, np.NaN, np.NaN, 2, np.NaN, np.NaN, np.NaN, np.NaN\n",
    "\n",
    "def removeOverlaps(segments_df):\n",
    "        \n",
    "    for day in segments_df.columns:\n",
    "\n",
    "        number_of_active_segments_on_the_day = len(segments_df[day].dropna())\n",
    "\n",
    "        if(number_of_active_segments_on_the_day > 1):\n",
    "                segments_df[day] = np.NaN\n",
    "\n",
    "    return segments_df        \n",
    "\n",
    "# collapse dataframe into single series\n",
    "# input df: \n",
    "# 3, np.NaN, np.NaN, np.NaN, 3, 3, 3, np.NaN\n",
    "# np.NaN, np.NaN, np.NaN, 2, np.NaN, np.NaN, np.NaN, np.NaN\n",
    "#\n",
    "# output: 3, np.NaN, np.NaN, 2, 3, 3, 3, np.NaN\n",
    "def collapseAllSegmentsIntoSingleSeries(segments_df):\n",
    "    \n",
    "    # get first non null location from the series\n",
    "    # if none found, return NA\n",
    "    def getLocationOrNA(location_series):\n",
    "\n",
    "        def notNaN(l):\n",
    "            return (not np.isnan(l))\n",
    "\n",
    "        for l in location_series.tolist():\n",
    "\n",
    "            if(notNaN(l)):\n",
    "                return l\n",
    "\n",
    "        return np.NaN\n",
    "\n",
    "    \n",
    "    # create an empty location series\n",
    "    number_of_days = len(segments_df.columns)\n",
    "    location_series = pd.Series([np.NaN]*number_of_days)\n",
    "    \n",
    "    # iterate over each day and update active location in a single series\n",
    "    for day in segments_df.columns:\n",
    "\n",
    "        location = getLocationOrNA(segments_df[day])\n",
    "        \n",
    "        location_series[day] = location \n",
    "        \n",
    "    return location_series\n",
    "\n",
    "\n",
    "# Join any two consecutive segments if there no segments in any other location inbetween \n",
    "# input df: \n",
    "# 3, 3, 3, np.NaN, np.NaN, 3, 3, 3, 3, 3, 3, 3, 3\n",
    "# np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 2, 2, 2, np.NaN, np.NaN, 2, 2, 2\n",
    "#\n",
    "# output df: \n",
    "# 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3\n",
    "# np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 2, 2, 2, np.NaN, np.NaN, 2, 2, 2\n",
    "\n",
    "def joinConsecutiveSegments(segments_df):\n",
    "\n",
    "    # Mark days which co-incide with a segment for another location\n",
    "    # input df: \n",
    "    # 3, 3, 3, np.NaN, np.NaN, 3, 3, 3, 3, np.NaN, 3, 3, 3\n",
    "    # np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 2, 2, 2, np.NaN, np.NaN, 2, 2, 2\n",
    "    #\n",
    "    # output df: \n",
    "    # 3, 3, 3, np.NaN, np.NaN, 3, 3, 3, 3, 3, 3, 3, 3\n",
    "    # np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 2, 2, 2, np.NaN, -99, 2, 2, 2\n",
    "    def highlightIneligibleLocationDays(segments_df):\n",
    "\n",
    "        for day in segments_df.columns:\n",
    "\n",
    "            number_of_active_segments_on_the_day = len(segments_df[day].dropna())\n",
    "\n",
    "            if(number_of_active_segments_on_the_day > 0):\n",
    "\n",
    "                segments_df[day] = segments_df[day].apply(lambda d: -99 if d != d else d)\n",
    "                \n",
    "    \n",
    "    # Un-mark days which co-incide with a segment for another location            \n",
    "    # input df: \n",
    "    # 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3\n",
    "    # np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 2, 2, 2, np.NaN, -99, 2, 2, 2\n",
    "    #\n",
    "    # output df: \n",
    "    # 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3\n",
    "    # np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 2, 2, 2, np.NaN, np.NaN, 2, 2, 2\n",
    "    def unhighlightLocationDays(segments_df):\n",
    "        \n",
    "        for column in segments_df:\n",
    "\n",
    "            segments_df[column] = segments_df[column].apply(lambda d: np.NaN if d == -99 else d)                \n",
    "            \n",
    "    # Join segments which do not have any ineligible days in between them  \n",
    "    # input df: \n",
    "    # 3, 3, 3, np.NaN, np.NaN, 3, 3, 3, 3, 3, 3, 3, 3\n",
    "    # np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 2, 2, 2, np.NaN, -99, 2, 2, 2\n",
    "    #\n",
    "    # output df: \n",
    "    # 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3\n",
    "    # np.NaN, np.NaN, np.NaN, np.NaN, np.NaN, 2, 2, 2, np.NaN, -99, 2, 2, 2\n",
    "    def joinSegmentsWithoutIneligibleDaysInBetween(location_series):\n",
    "\n",
    "        # get non-null location series            \n",
    "        locations = location_series.dropna()\n",
    "\n",
    "        # return if all locations are null    \n",
    "        if(locations.empty):\n",
    "            return location_series\n",
    "\n",
    "        # runner to track previous non-null location day\n",
    "        previous_active_day = locations.index[0]\n",
    "        # runner to track previous non-null location\n",
    "        previous_active_location = locations[previous_active_day]\n",
    "\n",
    "        # iterate through non-null days and join uninterrupted segments\n",
    "        for current_day in locations.index.tolist():\n",
    "\n",
    "            current_location = locations[current_day]\n",
    "\n",
    "            if((current_day - previous_active_day) > 1):\n",
    "\n",
    "                if(current_location == previous_active_location):\n",
    "\n",
    "                    location_series[previous_active_day:current_day] = current_location\n",
    "\n",
    "            previous_active_day = current_day\n",
    "            previous_active_location = current_location\n",
    "\n",
    "        return location_series\n",
    "    \n",
    "    # Mark days which co-incide with a segment for another location\n",
    "    highlightIneligibleLocationDays(segments_df)\n",
    "    \n",
    "    # Join segments which do not have any ineligible days in between them\n",
    "    segments_df = segments_df.apply(lambda location_series: joinSegmentsWithoutIneligibleDaysInBetween(location_series), axis = 1)\n",
    "    \n",
    "    # Un-mark days which co-incide with a segment for another location\n",
    "    unhighlightLocationDays(segments_df)\n",
    "    \n",
    "    return segments_df\n",
    "\n",
    "\n",
    "# create segments by carry forwarding location values\n",
    "# input: \n",
    "# location_series: 2, np.NaN, np.NaN, np.NaN, 3, np.NaN, np.NaN, 4, np.NaN\n",
    "# location_carry_forward_limit: 2\n",
    "#\n",
    "# output: 2, np.NaN, np.NaN, np.NaN, 3, 3, 3, 4, np.NaN\n",
    "def createSegments(location_series, location_carry_forward_limit):\n",
    "\n",
    "    # get non-null location series    \n",
    "    locations = location_series.dropna()\n",
    "\n",
    "    # return if all locations are null    \n",
    "    if(locations.empty):\n",
    "        return location_series\n",
    "    \n",
    "    # runner to track previous non-null location day\n",
    "    previous_active_day = locations.index[0]\n",
    "    # runner to track previous non-null location\n",
    "    previous_active_location = locations[previous_active_day]\n",
    "    \n",
    "    # iterate through non-null days and forward locations when needed\n",
    "    for current_day in locations.index.tolist():\n",
    "        \n",
    "        current_location = locations[current_day]\n",
    "        \n",
    "        missing_gap_length = current_day - previous_active_day - 1\n",
    "\n",
    "        if((missing_gap_length >= 1) and (missing_gap_length <= location_carry_forward_limit)):\n",
    "                \n",
    "            location_series[previous_active_day:current_day] = current_location                \n",
    "        \n",
    "        previous_active_day = current_day\n",
    "        previous_active_location = current_location\n",
    "        \n",
    "    return location_series\n",
    "    \n",
    "    \n",
    "# use daily modal locations to infer home location segments\n",
    "# input:\n",
    "# raw_location_series: pandas series representing daily locations of a user\n",
    "# location_carry_forward_limit: number of days to carry forward location\n",
    "# segment_proportion_days_threshold: proportion of days in a segment with modal location\n",
    "# minimum_segment_length: minimum segment length (in days)\n",
    "# minimum_stay_length: minimum number of days stayed at a location (in days)\n",
    "def computeHomeSegments(raw_location_series, location_carry_forward_limit, segment_proportion_days_threshold, minimum_segment_length, minimum_stay_length, drop_single_location_users):\n",
    "    \n",
    "    # dataframe to hold one series per unique location\n",
    "    all_segments_df = pd.DataFrame()\n",
    "    \n",
    "    # get all unique locations visited\n",
    "    unique_locations = raw_location_series.dropna().unique()\n",
    "    \n",
    "    if(len(unique_locations) == 0):\n",
    "        return raw_location_series\n",
    "    \n",
    "    # iterate over all unique locations\n",
    "    for current_loc in unique_locations:\n",
    "        \n",
    "        current_loc_raw = raw_location_series.copy()\n",
    "        \n",
    "        # get a data series containing only current location\n",
    "        current_loc_raw[current_loc_raw != current_loc] = np.NaN\n",
    "        \n",
    "        current_loc_segments = current_loc_raw.copy()\n",
    "    \n",
    "        # forward location to empty days upto carry_forward_limit\n",
    "        current_loc_segments = createSegments(current_loc_segments, location_carry_forward_limit)\n",
    "\n",
    "        # drop small segments\n",
    "        current_loc_segments = dropSmallSegments(current_loc_segments, minimum_segment_length)\n",
    "\n",
    "        # drop segments where proportion of raw_location/(raw_locations + forward_locations) is lower than threshold\n",
    "        current_loc_segments = dropLowPropSegments(current_loc_raw, current_loc_segments, segment_proportion_days_threshold)        \n",
    "        \n",
    "        # append series to dataframe if it contains any valid segments\n",
    "        if(current_loc_segments.notnull().any()):\n",
    "            all_segments_df = all_segments_df.append(current_loc_segments, ignore_index = True)\n",
    "        \n",
    "    # return location series with no segments\n",
    "    if(len(all_segments_df) == 0):\n",
    "        return pd.Series([np.NaN]*len(raw_location_series))\n",
    "    \n",
    "    # drop single location users\n",
    "    if((drop_single_location_users == True) and len(all_segments_df) == 1):\n",
    "        return pd.Series([np.NaN]*all_segments_df.shape[1])\n",
    "       \n",
    "    # join same location segments without any other location segments in between\n",
    "    all_segments_df = joinConsecutiveSegments(all_segments_df)\n",
    "    \n",
    "    # remove overlaps\n",
    "    all_segments_df = removeOverlaps(all_segments_df)\n",
    "    \n",
    "    # drop small segments meant to represent \"too-small\" stays\n",
    "    all_segments_df = dropSmallStays(all_segments_df, minimum_stay_length)\n",
    "    \n",
    "    # merge all segments into one series\n",
    "    all_segments = collapseAllSegmentsIntoSingleSeries(all_segments_df)\n",
    "        \n",
    "    return all_segments\n",
    "    \n",
    "\n",
    "def computeHomeLocationPerUser(daily_modal_locations, first_day, last_day, location_carry_forward_limit, segment_proportion_days_threshold, minimum_segment_length, minimum_stay_length, drop_single_location_users):\n",
    "\n",
    "    print(str(datetime.now()) + \" Reading raw dataset.\")\n",
    "\n",
    "    data = pd.read_csv(daily_modal_locations,\n",
    "                        dtype = {'phoneHash1': str,\n",
    "                                 'daily_modal_location': 'int16'},\n",
    "                        parse_dates = ['day'],\n",
    "                        date_parser = (lambda x: datetime.strptime(x, '%Y-%m-%d')))\n",
    "    \n",
    "    if(len(data) == 0):\n",
    "        \n",
    "        print(str(datetime.now()) + \" No records - Returning empty dataset.\")\n",
    "        return pd.DataFrame(columns = ['phoneHash1', 'day', 'home_location'])\n",
    "    \n",
    "    print(str(datetime.now()) + \" Converting dataset to row major format.\")\n",
    "    \n",
    "    # convert data to row major format for more efficient computing\n",
    "    data = convertToRowMajorFormat(data, first_day, last_day)\n",
    "    \n",
    "    print(str(datetime.now()) + \" Computing home segments.\")\n",
    "\n",
    "    data = ddf.from_pandas(data, npartitions = dask_n_workers*8)\n",
    "\n",
    "    # create meta for dask apply function\n",
    "    total_days = (last_day - first_day).days + 1\n",
    "    meta = {}\n",
    "    for t in range(0, total_days, 1):\n",
    "        meta[t] = 'int16'\n",
    "\n",
    "    data = data.apply(lambda user_daily_location: computeHomeSegments(user_daily_location, location_carry_forward_limit, segment_proportion_days_threshold, minimum_segment_length, minimum_stay_length, drop_single_location_users), \n",
    "                      axis = 1, \n",
    "                      meta=meta)\n",
    "    \n",
    "    # convert data back to column major form\n",
    "    data = convertToColumnMajorFormat(data, first_day, last_day)\n",
    "    \n",
    "    data = data.compute()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# first day of cdr activity\n",
    "first_day = datetime(2013,4,1)\n",
    "\n",
    "# last day of cdr activity\n",
    "last_day = datetime(2020,10,31)\n",
    "\n",
    "# number of days to carry forward location\n",
    "location_carry_forward_limit = 2\n",
    "\n",
    "# proportion of days in a segment with modal location\n",
    "segment_proportion_days_threshold = 0.5\n",
    "\n",
    "# minimum segment length (in days)\n",
    "minimum_segment_length = 7\n",
    "\n",
    "# minimum length of stay (in days)\n",
    "minimum_stay_length = 5\n",
    "\n",
    "# drop single location users\n",
    "drop_single_location_users = True\n",
    "\n",
    "input_data_dir = '/path/to/data/daily_modal_voice_version_bucketed/district_level/'\n",
    "output_data_dir = '/path/to/data/daily_modal_voice_only_2013-2020_version/'\n",
    "\n",
    "for file in os.listdir(input_data_dir):\n",
    "    \n",
    "    print(str(datetime.now()) + \" Using dataset .. \" + file)\n",
    "    \n",
    "    daily_modal_locations = input_data_dir + file\n",
    "\n",
    "    home_locations = computeHomeLocationPerUser(daily_modal_locations, first_day, last_day, location_carry_forward_limit, segment_proportion_days_threshold, minimum_segment_length, minimum_stay_length, drop_single_location_users)\n",
    "    \n",
    "    print(str(datetime.now()) + \" Saving result.\")\n",
    "    \n",
    "    home_locations.to_csv(output_data_dir + file, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-smehra_dask]",
   "language": "python",
   "name": "conda-env-.conda-smehra_dask-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
