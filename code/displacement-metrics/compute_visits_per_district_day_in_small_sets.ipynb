{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user specific python libraries to path\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/smehra/local-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(dashboard_address = 'localhost:7920', \n",
    "                       n_workers = 16, \n",
    "                       processes = True, \n",
    "                       threads_per_worker = 16,\n",
    "                       memory_limit = '16GB', \n",
    "                       local_directory = \"/data/tmp/smehra/tmp/dask-worker-space\")\n",
    "client = Client(cluster)\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as ddf\n",
    "\n",
    "# enable automated generational garbage collection\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import time, os\n",
    "from datetime import timedelta  \n",
    "from datetime import date\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# setup logging to a specified file\n",
    "log_file = '/data/tmp/smehra/logs/migration_metrics_computation.log'\n",
    "logging.basicConfig(filename=log_file,\n",
    "                            filemode='a+',\n",
    "                            format='%(asctime)s %(levelname)s %(message)s',\n",
    "                            datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                            level=logging.INFO)\n",
    "logger=logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_migrated_after_time_delta(origin_district, impact_day, impacted_users, total_impacted, time_delta):\n",
    "\n",
    "    if(((impact_day + time_delta) > 2770) or (total_impacted == 0)):\n",
    "        \n",
    "        empty_df = pd.DataFrame(np.NaN, columns=[\"migrated\"], index=district_ids)\n",
    "        return empty_df.to_dict()['migrated']\n",
    "    \n",
    "    users_in_other_districts_after_time_delta = users_per_district_day[(users_per_district_day.day_series == (impact_day + time_delta))].copy()\n",
    "\n",
    "    users_in_other_districts_after_time_delta['migrated'] = users_in_other_districts_after_time_delta.active_users.apply(lambda migrated: len(impacted_users.intersection(migrated)))\n",
    "    \n",
    "    users_in_other_districts_after_time_delta.set_index('district_id', inplace = True)\n",
    "    \n",
    "    return users_in_other_districts_after_time_delta[['migrated']].to_dict()['migrated']\n",
    "    \n",
    "\n",
    "def get_migration_metrics(district_day):\n",
    "    \n",
    "    origin_district = district_day[0]\n",
    "    impact_day = district_day[1]\n",
    "    \n",
    "    logger.info('origin_district: ' + str(origin_district)  + '. impact_day: ' + str(impact_day))\n",
    "    \n",
    "    impacted_users = users_per_district_day[(users_per_district_day.district_id == origin_district) & \n",
    "                                            (users_per_district_day.day_series == impact_day)].active_users.item()\n",
    "    total_impacted = len(impacted_users)\n",
    "\n",
    "    summary = get_total_migrated_after_time_delta(origin_district, impact_day, impacted_users, total_impacted, time_delta)\n",
    "    \n",
    "    summary['origin_district'] = origin_district\n",
    "    summary['impact_day'] = impact_day\n",
    "    summary['impacted'] = total_impacted\n",
    "    summary['visit_day'] = impact_day + time_delta\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# get a list of day_series values\n",
    "days = pd.DataFrame({'day_series': np.arange(1, 2771, 1).tolist()}) \n",
    "\n",
    "# get a list of district ids\n",
    "district_ids = sorted(gpd.read_file('/data/afg_anon/ShapeFiles/AFG_district_398/district398.shp').DISTID.tolist())\n",
    "districts = pd.DataFrame({'district_id': district_ids}) \n",
    "\n",
    "# get all district - day_series pairs in a dataframe\n",
    "district_day_all_permutations_df = districts.assign(dummy_col = 1).merge(days.assign(dummy_col = 1), how = 'outer').drop(columns = [\"dummy_col\"])\n",
    "district_day_all_permutations_list = district_day_all_permutations_df.to_numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute visits per district day at user id level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "user_location_directory = '/data/afg_anon/displacement_metrics/home_locations/daily_modal_voice_only_2013-2020_version/'\n",
    "\n",
    "for fname in os.listdir(user_location_directory):\n",
    "    \n",
    "    print(str(datetime.now()) + ' reading ' + fname)\n",
    "    logger.info('reading ' + fname)\n",
    "    \n",
    "    users_per_district_day = pd.read_csv(user_location_directory + fname,\n",
    "                                         dtype = {'phoneHash1': object,\n",
    "                                                  'home_location': 'int16',\n",
    "                                                  'day': object})\n",
    "    if(len(users_per_district_day) == 0):\n",
    "        print(str(datetime.now()) + ' Empty dataset. Skipped.')\n",
    "        logger.info('Empty dataset. Skipped.')\n",
    "        continue\n",
    "    \n",
    "    # get \"set\" of users for each district each day\n",
    "    users_per_district_day = users_per_district_day.groupby(['home_location', 'day'])['phoneHash1'].apply(set)\n",
    "    users_per_district_day = users_per_district_day.reset_index()\n",
    "    \n",
    "    # convert dates to day series\n",
    "    users_per_district_day['day'] = pd.to_datetime(users_per_district_day.day, format='%Y-%m-%d', errors='coerce')\n",
    "    users_per_district_day['day_series'] = users_per_district_day['day'].dt.date.apply(lambda d: (d - date(2013,3,31)).days)\n",
    "\n",
    "    # rename and reorder columns\n",
    "    users_per_district_day.rename(columns = {'home_location': 'district_id', 'phoneHash1': 'active_users'}, inplace = True)\n",
    "    users_per_district_day = users_per_district_day[['day_series', 'district_id', 'active_users']]\n",
    "    \n",
    "    # ensure we have a row for all permutations of district - day.\n",
    "    users_per_district_day = district_day_all_permutations_df.merge(users_per_district_day, on = ['district_id', 'day_series'], how = 'left')\n",
    "\n",
    "    # convert no users to empty set\n",
    "    users_per_district_day['active_users'] = users_per_district_day.active_users.apply(lambda u: set() if u != u else u)\n",
    "    \n",
    "    for time_delta in [30]:\n",
    "\n",
    "        print(str(datetime.now()) + ' Computing for time_delta = ' + str(time_delta))\n",
    "        logger.info('Computing for time_delta = ' + str(time_delta))\n",
    "        \n",
    "        with ThreadPool() as pool:\n",
    "\n",
    "            # get migration metrics for impacted users of each district - day\n",
    "            migration_metrics_as_list = pool.map(get_migration_metrics, district_day_all_permutations_list)\n",
    "        \n",
    "        # convert migration metrics list to dataframe\n",
    "        # schema - ['origin_district', 'impact_day', 'impacted', 'visit_day', 101, 102, 103, ... 3401]\n",
    "        migration_metrics_column_major = pd.DataFrame(migration_metrics_as_list)\n",
    "\n",
    "        print(str(datetime.now()) + ' Unpivoting dataset.')\n",
    "        logger.info('Unpivoting dataset.')\n",
    "\n",
    "        # convert to row major form\n",
    "        # schema - ['origin_district', 'impact_day', 'impacted', 'visit_day', 'destination_district', 'visits']\n",
    "        migration_metrics = migration_metrics_column_major.melt(id_vars=['origin_district', 'impact_day', 'impacted', 'visit_day'], var_name='destination_district', value_name='visits')\n",
    "        migration_metrics = migration_metrics.sort_values(['origin_district', 'impact_day']).reset_index(drop = True)\n",
    "        migration_metrics = migration_metrics[['origin_district', 'impact_day', 'impacted', 'destination_district', 'visit_day', 'visits']]\n",
    "\n",
    "        print(str(datetime.now()) + ' Saving dataset.')\n",
    "        logger.info('Saving dataset.')\n",
    "        migration_metrics.to_csv('/data/tmp/smehra/aggregated_data/poppy/visits_per_district_day/time_delta_' + str(time_delta) + '_days/user_id_level/' + fname, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert user id level metrics to impact day level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_delta = 30\n",
    "    \n",
    "visits_dataset_directory = '/data/tmp/smehra/aggregated_data/poppy/visits_per_district_day/time_delta_' + str(time_delta) + '_days/user_id_level/'\n",
    "filepaths = [visits_dataset_directory + f for f in os.listdir(visits_dataset_directory)]\n",
    "\n",
    "for impact_day in range(1, 2771):\n",
    "\n",
    "    print(str(datetime.now()) + ' Computing for impact day ' + str(impact_day))\n",
    "\n",
    "    user_id_level_datasets = []\n",
    "\n",
    "    for file in filepaths:\n",
    "\n",
    "        # read user_id level dataset\n",
    "        user_id_level_data = ddf.read_csv(file,\n",
    "                                          dtype = {'origin_district': 'int16',\n",
    "                                                   'impact_day': 'int16',\n",
    "                                                   'impacted': 'int32',\n",
    "                                                   'destination_district': 'int16',\n",
    "                                                   'visit_day': 'int16',\n",
    "                                                   'visits': 'float64'})\n",
    "\n",
    "        # keep only data for a specific impact_day\n",
    "        user_id_level_data = user_id_level_data[user_id_level_data.impact_day == impact_day]\n",
    "\n",
    "        # add it to the list of dataframes\n",
    "        user_id_level_datasets.append(user_id_level_data)\n",
    "\n",
    "    # concatenate together data for a specific impact_day from all user id datasets\n",
    "    impact_day_level_data = ddf.concat(user_id_level_datasets)            \n",
    "    impact_day_level_data.to_csv('/data/tmp/smehra/aggregated_data/poppy/visits_per_district_day/time_delta_' + str(time_delta) + '_days/impact_day_level/impact_day_' + str(impact_day), index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate impact day level metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_delta = 30\n",
    "    \n",
    "datasets_directory = '/data/tmp/smehra/aggregated_data/poppy/visits_per_district_day/time_delta_' + str(time_delta) + '_days/impact_day_level/'\n",
    "\n",
    "for impact_day in range(1, 2771):\n",
    "    \n",
    "    print(str(datetime.now()) + ' Computing for impact day ' + str(impact_day))\n",
    "    impact_day_folder = datasets_directory + 'impact_day_' + str(impact_day)\n",
    "    \n",
    "    # read impact_day level dataset\n",
    "    impact_day_data = ddf.read_csv(impact_day_folder + '/*',\n",
    "                                         dtype = {'origin_district': 'int16',\n",
    "                                                  'impact_day': 'int16',\n",
    "                                                  'impacted': 'int32',\n",
    "                                                  'destination_district': 'int16',\n",
    "                                                  'visit_day': 'int16',\n",
    "                                                  'visits': 'float64'})\n",
    "\n",
    "    # calculate sum of \"impacted\" users for each group\n",
    "    impacted_agg = impact_day_data.groupby(['origin_district', 'impact_day', 'destination_district', 'visit_day'])['impacted'].sum().reset_index()\n",
    "\n",
    "    # drop rows where impacted_users count were zero i.e visits were NA\n",
    "    visits_agg = impact_day_data[~impact_day_data.visits.isna()]\n",
    "    # calculate sum of \"visits\" for each group\n",
    "    visits_agg = visits_agg.groupby(['origin_district', 'impact_day', 'destination_district', 'visit_day'])['visits'].sum().reset_index()\n",
    "    \n",
    "    # merge to have total impacted and visits count in same dataset\n",
    "    impacted_and_visits_merged = impacted_agg.merge(visits_agg, on = ['origin_district', 'impact_day', 'destination_district', 'visit_day'], how = 'left')\n",
    "    \n",
    "    impacted_and_visits_merged.to_csv('/data/tmp/smehra/aggregated_data/poppy/visits_per_district_day/time_delta_' + str(time_delta) + '_days/aggregated/impact_day_' + str(impact_day) + '.csv', index = False, single_file = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-smehra_dask]",
   "language": "python",
   "name": "conda-env-.conda-smehra_dask-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
